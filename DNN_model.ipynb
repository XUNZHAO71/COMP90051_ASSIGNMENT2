{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Domain Adversarial Neural Network (DANN) for Text Generation Detection\n",
    "# This notebook implements an optimized DANN model for detecting machine-generated vs human-written text across different domains.\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training datasets...\n",
      "Domain 1: 1000 samples, Class distribution: [500 500]\n",
      "Domain 2: 5000 samples, Class distribution: [ 250 4750]\n"
     ]
    }
   ],
   "source": [
    "def load_json_data(path):\n",
    "    \"\"\"Load training data from JSON file and convert to text format for vectorization\"\"\"\n",
    "    texts, labels = [], []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            sample = json.loads(line)\n",
    "            # Convert token indices to string for vectorizer\n",
    "            token_str = ' '.join(map(str, sample['text']))\n",
    "            texts.append(token_str)\n",
    "            labels.append(sample['label'])\n",
    "    return texts, labels\n",
    "\n",
    "def load_test_data(path):\n",
    "    \"\"\"Load test data and return texts with IDs\"\"\"\n",
    "    test_texts, test_ids = [], []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            sample = json.loads(line)\n",
    "            text = ' '.join(map(str, sample['text']))\n",
    "            test_texts.append(text)\n",
    "            test_ids.append(sample['id'])\n",
    "    return test_texts, test_ids\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading training datasets...\")\n",
    "X1, y1 = load_json_data('domain1_train_data.json')\n",
    "X2, y2 = load_json_data('domain2_train_data.json')\n",
    "\n",
    "print(f\"Domain 1: {len(X1)} samples, Class distribution: {np.bincount(y1)}\")\n",
    "print(f\"Domain 2: {len(X2)} samples, Class distribution: {np.bincount(y2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High level of feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating advanced feature vectors...\n",
      "Feature dimensions: 15000\n"
     ]
    }
   ],
   "source": [
    "# Split domains into train/validation\n",
    "X1_train, X1_val, y1_train, y1_val = train_test_split(X1, y1, test_size=0.2, random_state=42, stratify=y1)\n",
    "X2_train, X2_val, y2_train, y2_val = train_test_split(X2, y2, test_size=0.2, random_state=42, stratify=y2)\n",
    "\n",
    "# Advanced vectorization with both Count and TF-IDF features\n",
    "print(\"Creating advanced feature vectors...\")\n",
    "\n",
    "# Count vectorizer for basic term frequency\n",
    "count_vectorizer = CountVectorizer(max_features=10000, ngram_range=(1, 2))\n",
    "X_train_combined = X1_train + X2_train\n",
    "count_vectorizer.fit(X_train_combined)\n",
    "\n",
    "# TF-IDF vectorizer for weighted features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "tfidf_vectorizer.fit(X_train_combined)\n",
    "\n",
    "# Transform training data\n",
    "X1_train_count = count_vectorizer.transform(X1_train)\n",
    "X1_train_tfidf = tfidf_vectorizer.transform(X1_train)\n",
    "X2_train_count = count_vectorizer.transform(X2_train)\n",
    "X2_train_tfidf = tfidf_vectorizer.transform(X2_train)\n",
    "\n",
    "# Combine count and TF-IDF features\n",
    "from scipy.sparse import hstack\n",
    "X1_train_vec = hstack([X1_train_count, X1_train_tfidf])\n",
    "X2_train_vec = hstack([X2_train_count, X2_train_tfidf])\n",
    "\n",
    "# Transform validation data\n",
    "X1_val_vec = hstack([count_vectorizer.transform(X1_val), tfidf_vectorizer.transform(X1_val)])\n",
    "X2_val_vec = hstack([count_vectorizer.transform(X2_val), tfidf_vectorizer.transform(X2_val)])\n",
    "\n",
    "print(f\"Feature dimensions: {X1_train_vec.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE to deal with the imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying SMOTE to balance Domain 2...\n",
      "Domain 2 after SMOTE: 7600 samples, Class distribution: [3800 3800]\n"
     ]
    }
   ],
   "source": [
    "# Apply SMOTE to balance Domain 2 class distribution\n",
    "print(\"Applying SMOTE to balance Domain 2...\")\n",
    "smote = SMOTE(random_state=42, k_neighbors=3)\n",
    "X2_train_balanced, y2_train_balanced = smote.fit_resample(X2_train_vec.toarray(), y2_train)\n",
    "\n",
    "print(f\"Domain 2 after SMOTE: {len(y2_train_balanced)} samples, Class distribution: {np.bincount(y2_train_balanced)}\")\n",
    "\n",
    "# Convert to numpy arrays for PyTorch\n",
    "X1_train_np = X1_train_vec.toarray().astype(np.float32)\n",
    "X2_train_np = X2_train_balanced.astype(np.float32)\n",
    "X1_val_np = X1_val_vec.toarray().astype(np.float32)\n",
    "X2_val_np = X2_val_vec.toarray().astype(np.float32)\n",
    "\n",
    "y1_train_np = np.array(y1_train, dtype=np.int64)\n",
    "y2_train_np = np.array(y2_train_balanced, dtype=np.int64)\n",
    "y1_val_np = np.array(y1_val, dtype=np.int64)\n",
    "y2_val_np = np.array(y2_val, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure of the DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversalLayer(torch.autograd.Function):\n",
    "    \"\"\"Gradient Reversal Layer for adversarial training\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return -ctx.alpha * grad_output, None\n",
    "\n",
    "class AdvancedDANN(nn.Module):\n",
    "    \"\"\"Advanced Domain Adversarial Neural Network with improved architecture\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims=[512, 256, 128], num_classes=2, num_domains=2, dropout_rate=0.5):\n",
    "        super(AdvancedDANN, self).__init__()\n",
    "        \n",
    "        # Advanced feature extractor with residual connections\n",
    "        self.feature_extractor = nn.ModuleList()\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            self.feature_extractor.append(nn.Sequential(\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Residual connection\n",
    "        self.residual_projection = nn.Linear(input_dim, hidden_dims[-1])\n",
    "        \n",
    "        # Label classifier with attention mechanism\n",
    "        self.label_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[-1], hidden_dims[-1] // 2),\n",
    "            nn.BatchNorm1d(hidden_dims[-1] // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate * 0.5),\n",
    "            nn.Linear(hidden_dims[-1] // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Domain classifier\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[-1], hidden_dims[-1] // 2),\n",
    "            nn.BatchNorm1d(hidden_dims[-1] // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate * 0.5),\n",
    "            nn.Linear(hidden_dims[-1] // 2, num_domains)\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism for features\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[-1], hidden_dims[-1]),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dims[-1], 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, alpha=1.0):\n",
    "        # Feature extraction with residual connection\n",
    "        features = x\n",
    "        for layer in self.feature_extractor:\n",
    "            features = layer(features)\n",
    "        \n",
    "        # Add residual connection\n",
    "        residual = self.residual_projection(x)\n",
    "        features = features + residual\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        attention_weights = self.attention(features)\n",
    "        attended_features = features * attention_weights\n",
    "        \n",
    "        # Label prediction\n",
    "        label_output = self.label_classifier(attended_features)\n",
    "        \n",
    "        # Domain prediction with gradient reversal\n",
    "        reversed_features = GradientReversalLayer.apply(attended_features, alpha)\n",
    "        domain_output = self.domain_classifier(reversed_features)\n",
    "        \n",
    "        return label_output, domain_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-level training strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedBatchSampler:\n",
    "    \"\"\"Custom batch sampler for balanced domain training\"\"\"\n",
    "    \n",
    "    def __init__(self, X1, y1, d1, X2, y2, d2, batch_size):\n",
    "        self.X1, self.y1, self.d1 = X1, y1, d1\n",
    "        self.X2, self.y2, self.d2 = X2, y2, d2\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            # Sample equal amounts from each domain\n",
    "            X1_indices = np.random.choice(len(self.X1), self.batch_size // 2, replace=True)\n",
    "            X2_indices = np.random.choice(len(self.X2), self.batch_size // 2, replace=True)\n",
    "            \n",
    "            X_batch = np.vstack([self.X1[X1_indices], self.X2[X2_indices]])\n",
    "            y_batch = np.hstack([self.y1[X1_indices], self.y2[X2_indices]])\n",
    "            d_batch = np.hstack([self.d1[X1_indices], self.d2[X2_indices]])\n",
    "            \n",
    "            yield torch.FloatTensor(X_batch), torch.LongTensor(y_batch), torch.LongTensor(d_batch)\n",
    "\n",
    "def evaluate_model(model, X1_val, y1_val, X2_val, y2_val):\n",
    "    \"\"\"Evaluate model on validation sets\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Domain 1 validation\n",
    "        label_out1, _ = model(X1_val)\n",
    "        pred1 = torch.argmax(label_out1, dim=1)\n",
    "        acc1 = accuracy_score(y1_val.numpy(), pred1.numpy())\n",
    "        \n",
    "        # Domain 2 validation\n",
    "        label_out2, _ = model(X2_val)\n",
    "        pred2 = torch.argmax(label_out2, dim=1)\n",
    "        acc2 = accuracy_score(y2_val.numpy(), pred2.numpy())\n",
    "        \n",
    "        # Combined accuracy\n",
    "        all_preds = torch.cat([pred1, pred2])\n",
    "        all_labels = torch.cat([y1_val, y2_val])\n",
    "        combined_acc = accuracy_score(all_labels.numpy(), all_preds.numpy())\n",
    "    \n",
    "    model.train()\n",
    "    return combined_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_advanced_dann(model, X1_train, y1_train, X2_train, y2_train, \n",
    "                       X1_val, y1_val, X2_val, y2_val, \n",
    "                       num_epochs=50, batch_size=64, lr=0.001):\n",
    "    \"\"\"Advanced training with curriculum learning and adaptive alpha\"\"\"\n",
    "    \n",
    "    # Prepare domain labels\n",
    "    d1_train = np.zeros(len(y1_train), dtype=np.int64)  # Domain 1\n",
    "    d2_train = np.ones(len(y2_train), dtype=np.int64)   # Domain 2\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X1_val_tensor = torch.FloatTensor(X1_val)\n",
    "    X2_val_tensor = torch.FloatTensor(X2_val)\n",
    "    y1_val_tensor = torch.LongTensor(y1_val)\n",
    "    y2_val_tensor = torch.LongTensor(y2_val)\n",
    "    \n",
    "    # Initialize optimizer with different learning rates\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': model.feature_extractor.parameters(), 'lr': lr},\n",
    "        {'params': model.label_classifier.parameters(), 'lr': lr},\n",
    "        {'params': model.domain_classifier.parameters(), 'lr': lr * 0.1},\n",
    "        {'params': model.attention.parameters(), 'lr': lr * 0.5}\n",
    "    ], weight_decay=1e-4)\n",
    "    \n",
    "    # Loss functions with class weights\n",
    "    label_weights = torch.FloatTensor([1.0, 1.0])  # Balanced for labels\n",
    "    domain_weights = torch.FloatTensor([5.0, 1.0])  # Higher weight for Domain 1\n",
    "    \n",
    "    criterion_label = nn.CrossEntropyLoss(weight=label_weights)\n",
    "    criterion_domain = nn.CrossEntropyLoss(weight=domain_weights)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    batch_sampler = BalancedBatchSampler(X1_train, y1_train, d1_train, X2_train, y2_train, d2_train, batch_size)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        total_label_loss = 0.0\n",
    "        total_domain_loss = 0.0\n",
    "        \n",
    "        # Adaptive alpha with curriculum learning\n",
    "        p = float(epoch) / num_epochs\n",
    "        alpha = 2.0 / (1.0 + np.exp(-10 * p)) - 1.0\n",
    "        \n",
    "        # Training batches per epoch\n",
    "        batches_per_epoch = max(len(X1_train), len(X2_train)) // batch_size\n",
    "        \n",
    "        for batch_idx in range(batches_per_epoch):\n",
    "            X_batch, y_batch, d_batch = next(iter(batch_sampler))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            label_output, domain_output = model(X_batch, alpha)\n",
    "            \n",
    "            # Compute losses\n",
    "            label_loss = criterion_label(label_output, y_batch)\n",
    "            domain_loss = criterion_domain(domain_output, d_batch)\n",
    "            total_batch_loss = label_loss + domain_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            total_batch_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += total_batch_loss.item()\n",
    "            total_label_loss += label_loss.item()\n",
    "            total_domain_loss += domain_loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            val_acc = evaluate_model(model, X1_val_tensor, y1_val_tensor, X2_val_tensor, y2_val_tensor)\n",
    "            scheduler.step(val_acc)\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/batches_per_epoch:.4f}, \"\n",
    "                  f\"Label Loss: {total_label_loss/batches_per_epoch:.4f}, \"\n",
    "                  f\"Domain Loss: {total_domain_loss/batches_per_epoch:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, Alpha: {alpha:.4f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                torch.save(model.state_dict(), 'best_dann_model.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= 10:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_dann_model.pth'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model initializing and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimension: 15000\n",
      "Model parameters: 19993861\n",
      "\n",
      "Starting advanced DANN training...\n",
      "Epoch [5/100], Loss: 0.7955, Label Loss: 0.0149, Domain Loss: 0.7806, Val Acc: 0.9583, Alpha: 0.1974\n",
      "Epoch [10/100], Loss: 0.6568, Label Loss: 0.0165, Domain Loss: 0.6404, Val Acc: 0.9642, Alpha: 0.4219\n",
      "Epoch [15/100], Loss: 0.6234, Label Loss: 0.0220, Domain Loss: 0.6014, Val Acc: 0.9650, Alpha: 0.6044\n",
      "Epoch [20/100], Loss: 0.6268, Label Loss: 0.0150, Domain Loss: 0.6119, Val Acc: 0.9592, Alpha: 0.7398\n",
      "Epoch [25/100], Loss: 0.6459, Label Loss: 0.0149, Domain Loss: 0.6310, Val Acc: 0.9667, Alpha: 0.8337\n",
      "Epoch [30/100], Loss: 0.5801, Label Loss: 0.0156, Domain Loss: 0.5645, Val Acc: 0.9667, Alpha: 0.8957\n",
      "Epoch [35/100], Loss: 0.6211, Label Loss: 0.0146, Domain Loss: 0.6065, Val Acc: 0.9650, Alpha: 0.9354\n",
      "Epoch [40/100], Loss: 0.6169, Label Loss: 0.0059, Domain Loss: 0.6110, Val Acc: 0.9658, Alpha: 0.9603\n",
      "Epoch [45/100], Loss: 0.6085, Label Loss: 0.0129, Domain Loss: 0.5957, Val Acc: 0.9625, Alpha: 0.9757\n",
      "Epoch [50/100], Loss: 0.5945, Label Loss: 0.0055, Domain Loss: 0.5890, Val Acc: 0.9658, Alpha: 0.9852\n",
      "Epoch [55/100], Loss: 0.5887, Label Loss: 0.0062, Domain Loss: 0.5825, Val Acc: 0.9633, Alpha: 0.9910\n",
      "Epoch [60/100], Loss: 0.5919, Label Loss: 0.0102, Domain Loss: 0.5817, Val Acc: 0.9658, Alpha: 0.9945\n",
      "Epoch [65/100], Loss: 0.5627, Label Loss: 0.0011, Domain Loss: 0.5616, Val Acc: 0.9625, Alpha: 0.9967\n",
      "Epoch [70/100], Loss: 0.5816, Label Loss: 0.0041, Domain Loss: 0.5774, Val Acc: 0.9692, Alpha: 0.9980\n",
      "Epoch [75/100], Loss: 0.5448, Label Loss: 0.0045, Domain Loss: 0.5403, Val Acc: 0.9675, Alpha: 0.9988\n",
      "Epoch [80/100], Loss: 0.5810, Label Loss: 0.0046, Domain Loss: 0.5764, Val Acc: 0.9675, Alpha: 0.9993\n",
      "Epoch [85/100], Loss: 0.5535, Label Loss: 0.0012, Domain Loss: 0.5523, Val Acc: 0.9675, Alpha: 0.9996\n",
      "Epoch [90/100], Loss: 0.5235, Label Loss: 0.0027, Domain Loss: 0.5208, Val Acc: 0.9650, Alpha: 0.9997\n",
      "Epoch [95/100], Loss: 0.5558, Label Loss: 0.0037, Domain Loss: 0.5521, Val Acc: 0.9633, Alpha: 0.9998\n",
      "Epoch [100/100], Loss: 0.5230, Label Loss: 0.0018, Domain Loss: 0.5212, Val Acc: 0.9642, Alpha: 0.9999\n"
     ]
    }
   ],
   "source": [
    "# Initialize the advanced model\n",
    "input_dim = X1_train_np.shape[1]\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "\n",
    "# Model hyperparameters\n",
    "model = AdvancedDANN(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dims=[1024, 512, 256],  # Larger network\n",
    "    num_classes=2,\n",
    "    num_domains=2,\n",
    "    dropout_rate=0.3\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(\"\\nStarting advanced DANN training...\")\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_advanced_dann(\n",
    "    model, \n",
    "    X1_train_np, y1_train_np, \n",
    "    X2_train_np, y2_train_np,\n",
    "    X1_val_np, y1_val_np,\n",
    "    X2_val_np, y2_val_np,\n",
    "    num_epochs=100,\n",
    "    batch_size=128,\n",
    "    lr=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final Model Evaluation ===\n",
      "\n",
      "Domain 1 Results:\n",
      "Label Classification Accuracy: 0.9100\n",
      "Domain Confusion Rate: 0.0000 (lower is better)\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Human       0.96      0.86      0.91       100\n",
      "     Machine       0.87      0.96      0.91       100\n",
      "\n",
      "    accuracy                           0.91       200\n",
      "   macro avg       0.91      0.91      0.91       200\n",
      "weighted avg       0.91      0.91      0.91       200\n",
      "\n",
      "\n",
      "Domain 2 Results:\n",
      "Label Classification Accuracy: 0.9810\n",
      "Domain Confusion Rate: 1.0000 (lower is better)\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Human       0.88      0.72      0.79        50\n",
      "     Machine       0.99      0.99      0.99       950\n",
      "\n",
      "    accuracy                           0.98      1000\n",
      "   macro avg       0.93      0.86      0.89      1000\n",
      "weighted avg       0.98      0.98      0.98      1000\n",
      "\n",
      "\n",
      "=== Overall Performance ===\n",
      "Average Accuracy: 0.9455\n",
      "Domain Invariance Score: 0.5000\n",
      "Balanced Score: 1.4183\n"
     ]
    }
   ],
   "source": [
    "def detailed_evaluation(model, X1, y1, X2, y2, domain_names=['Domain 1', 'Domain 2']):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for i, (X, y, domain_name) in enumerate([(X1, y1, domain_names[0]), (X2, y2, domain_names[1])]):\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X) if not isinstance(X, torch.Tensor) else X\n",
    "            y_tensor = torch.LongTensor(y) if not isinstance(y, torch.Tensor) else y\n",
    "            \n",
    "            label_output, domain_output = model(X_tensor)\n",
    "            predictions = torch.argmax(label_output, dim=1)\n",
    "            domain_predictions = torch.argmax(domain_output, dim=1)\n",
    "            \n",
    "            accuracy = accuracy_score(y_tensor.numpy(), predictions.numpy())\n",
    "            report = classification_report(y_tensor.numpy(), predictions.numpy(), \n",
    "                                         target_names=['Human', 'Machine'], output_dict=True)\n",
    "            \n",
    "            # Domain classification accuracy\n",
    "            domain_labels = torch.full_like(y_tensor, i)  # 0 for domain 1, 1 for domain 2\n",
    "            domain_acc = accuracy_score(domain_labels.numpy(), domain_predictions.numpy())\n",
    "            \n",
    "            results[domain_name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'classification_report': report,\n",
    "                'domain_confusion': 1 - domain_acc  # Lower is better (more domain-invariant)\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{domain_name} Results:\")\n",
    "            print(f\"Label Classification Accuracy: {accuracy:.4f}\")\n",
    "            print(f\"Domain Confusion Rate: {1-domain_acc:.4f} (lower is better)\")\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(y_tensor.numpy(), predictions.numpy(), \n",
    "                                       target_names=['Human', 'Machine']))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate on validation sets\n",
    "print(\"=== Final Model Evaluation ===\")\n",
    "val_results = detailed_evaluation(trained_model, X1_val_np, y1_val_np, X2_val_np, y2_val_np)\n",
    "\n",
    "# Calculate overall metrics\n",
    "overall_acc = (val_results['Domain 1']['accuracy'] + val_results['Domain 2']['accuracy']) / 2\n",
    "domain_invariance = (val_results['Domain 1']['domain_confusion'] + val_results['Domain 2']['domain_confusion']) / 2\n",
    "\n",
    "print(f\"\\n=== Overall Performance ===\")\n",
    "print(f\"Average Accuracy: {overall_acc:.4f}\")\n",
    "print(f\"Domain Invariance Score: {domain_invariance:.4f}\")\n",
    "print(f\"Balanced Score: {overall_acc * (1 + domain_invariance):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the data predictions and make the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "Test data shape: torch.Size([4000, 15000])\n",
      "\n",
      "Predictions saved to 'advanced_dann_predictions.csv'\n",
      "Prediction distribution: [1073 2927]\n",
      "Average confidence: 0.9933\n",
      "\n",
      "First 10 predictions:\n",
      "   id  class\n",
      "0   0      1\n",
      "1   1      0\n",
      "2   2      0\n",
      "3   3      1\n",
      "4   4      0\n",
      "5   5      1\n",
      "6   6      0\n",
      "7   7      1\n",
      "8   8      1\n",
      "9   9      1\n"
     ]
    }
   ],
   "source": [
    "# Load and process test data\n",
    "print(\"Loading test data...\")\n",
    "X_test_raw, test_ids = load_test_data('test_data.json')\n",
    "\n",
    "# Transform test data using the same vectorizers\n",
    "X_test_count = count_vectorizer.transform(X_test_raw)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_raw)\n",
    "X_test_combined = hstack([X_test_count, X_test_tfidf])\n",
    "X_test_tensor = torch.FloatTensor(X_test_combined.toarray())\n",
    "\n",
    "print(f\"Test data shape: {X_test_tensor.shape}\")\n",
    "\n",
    "# Generate predictions\n",
    "trained_model.eval()\n",
    "with torch.no_grad():\n",
    "    label_output, _ = trained_model(X_test_tensor)\n",
    "    probabilities = F.softmax(label_output, dim=1)\n",
    "    predictions = torch.argmax(label_output, dim=1).tolist()\n",
    "    confidence_scores = torch.max(probabilities, dim=1)[0].tolist()\n",
    "\n",
    "# Create submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'class': predictions\n",
    "})\n",
    "\n",
    "# Save predictions\n",
    "submission_df.to_csv('advanced_dann_predictions.csv', index=False)\n",
    "print(f\"\\nPredictions saved to 'advanced_dann_predictions.csv'\")\n",
    "print(f\"Prediction distribution: {np.bincount(predictions)}\")\n",
    "print(f\"Average confidence: {np.mean(confidence_scores):.4f}\")\n",
    "\n",
    "# Display first few predictions\n",
    "print(\"\\nFirst 10 predictions:\")\n",
    "print(submission_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
